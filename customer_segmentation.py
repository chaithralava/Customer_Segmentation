# -*- coding: utf-8 -*-
"""Customer_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ie_30AwiEPhRlOkIhpUVz82yVpRNedkh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Loading the dataset
data = pd.read_csv('/content/Mall_Customers.csv')

# Display the first few rows
data.head()

# Check for missing values
data.isnull().sum()

# Drop or fill missing values (depending on the dataset)
data.dropna(inplace=True)

# Selecting relevant features
features = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# Optional: Add other features if necessary

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Scatter plot of the selected features
plt.figure(figsize=(10, 6))
plt.scatter(scaled_features[:, 0], scaled_features[:, 1], s=100, c='blue', marker='o', alpha=0.5)
plt.title('Customer Distribution')
plt.xlabel('Annual Income (scaled)')
plt.ylabel('Spending Score (scaled)')
plt.show()

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Apply K-Means with the chosen number of clusters
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(scaled_features)

# Add the cluster labels to the original data
data['Cluster'] = clusters

# Calculate the Silhouette Score
silhouette_avg = silhouette_score(scaled_features, clusters)
print(f'Silhouette Score: {silhouette_avg}')

plt.figure(figsize=(10, 6))
plt.scatter(scaled_features[:, 0], scaled_features[:, 1], c=clusters, cmap='rainbow', s=100, alpha=0.7)
plt.title('Customer Segments')
plt.xlabel('Annual Income (scaled)')
plt.ylabel('Spending Score (scaled)')
plt.show()

# Check the data types of all columns
data.dtypes

# Selecting only numeric columns for profiling
numeric_data = data.select_dtypes(include=[np.number])

# Ensure only numeric columns are included in the groupby operation
numeric_features = ['Annual Income (k$)', 'Spending Score (1-100)']  # Update this list with your numeric features
cluster_profile = data.groupby('Cluster')[numeric_features].mean()

# Display the cluster profiles
print(cluster_profile)

from sklearn.preprocessing import LabelEncoder

# Example: Encoding a categorical column
label_encoder = LabelEncoder()
data['Gender'] = label_encoder.fit_transform(data['Gender'])

# Or use one-hot encoding
data = pd.get_dummies(data, columns=['Gender'], drop_first=True)

# Re-check the data types to ensure correctness
data.dtypes

# Re-run cluster profiling
cluster_profile = data.groupby('Cluster').mean()

# Display the cluster profiles
print(cluster_profile)


# Grouping data by clusters to calculate the mean values of each feature
cluster_profile = data.groupby('Cluster').mean()

# Display the cluster profiles
cluster_profile

# Bar plot for the average Annual Income per cluster
plt.figure(figsize=(10, 6))
sns.barplot(x=cluster_profile.index, y=cluster_profile['Annual Income (k$)'])
plt.title('Average Annual Income by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Annual Income (k$)')
plt.show()

# Bar plot for the average Spending Score per cluster
plt.figure(figsize=(10, 6))
sns.barplot(x=cluster_profile.index, y=cluster_profile['Spending Score (1-100)'])
plt.title('Average Spending Score by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Spending Score')
plt.show()

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_features = pca.fit_transform(scaled_features)

plt.figure(figsize=(10, 6))
plt.scatter(pca_features[:, 0], pca_features[:, 1], c=clusters, cmap='rainbow', s=100, alpha=0.7)
plt.title('PCA of Customer Segments')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.show()

# Additional Libraries
from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score
import seaborn as sns

# 1. Evaluating Cluster Quality with Additional Metrics

# Calculate Davies-Bouldin Score
db_score = davies_bouldin_score(scaled_features, clusters)
print(f'Davies-Bouldin Score: {db_score}')

# Calculate Calinski-Harabasz Score
ch_score = calinski_harabasz_score(scaled_features, clusters)
print(f'Calinski-Harabasz Score: {ch_score}')

# 2. Analyzing Feature Distributions within Clusters

# Visualize the distribution of features for each cluster
plt.figure(figsize=(12, 6))
sns.boxplot(x='Cluster', y='Annual Income (k$)', data=data)
plt.title('Annual Income Distribution by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Annual Income (k$)')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x='Cluster', y='Spending Score (1-100)', data=data)
plt.title('Spending Score Distribution by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Spending Score (1-100)')
plt.show()

# 3. Correlation Analysis within Clusters

# Calculate correlation matrix and visualize it with a heatmap
cluster_corr = data.groupby('Cluster')[numeric_features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(cluster_corr, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Features by Cluster')
plt.show()

# 4. Pair Plot of Clusters for All Numeric Features

# Adding pair plot for all numeric features, colored by cluster
sns.pairplot(data, vars=numeric_features, hue='Cluster', palette='Set1', diag_kind='kde', plot_kws={'alpha':0.6, 's':80})
plt.suptitle('Pair Plot of Numeric Features by Cluster', y=1.02)
plt.show()

# 5. Additional Cluster Profiling with Variance and Descriptions

# Calculate standard deviation within each cluster for profiling feature variation
cluster_variance = data.groupby('Cluster')[numeric_features].std()
print("Cluster Feature Variance:\n", cluster_variance)

# 6. Average Feature Values per Cluster (Profile Summary)

# Use existing cluster profile to summarize clusters with custom labels
cluster_profile = data.groupby('Cluster')[numeric_features].mean()
for cluster_id, profile in cluster_profile.iterrows():
    print(f'\nCluster {cluster_id} Summary:')
    print(f' - Average Annual Income: ${profile["Annual Income (k$)"]:.2f}k')
    print(f' - Average Spending Score: {profile["Spending Score (1-100)"]:.2f}')
    print(f' - Variability in Income: {cluster_variance.loc[cluster_id, "Annual Income (k$)"]:.2f}')
    print(f' - Variability in Spending Score: {cluster_variance.loc[cluster_id, "Spending Score (1-100)"]:.2f}')

# 7. Saving Clustered Data for Future Use
# Saving the data with cluster labels if needed
data.to_csv('clustered_customer_data.csv', index=False)